{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Build and Test a Mini RAG System from Scratch ğŸ§ "
      ],
      "metadata": {
        "id": "Fg7IRyVjzb74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers transformers torch"
      ],
      "metadata": {
        "id": "NCUjET-gzbee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âš™ï¸ Part 1: The Retriever - Finding the Right Knowledge\n",
        "\n",
        "First, we'll set up our Retriever. Its job is to take a question and find the most relevant piece of text from our knowledge base.\n",
        "\n",
        "1.  **Load the Embedding Model** (`all-MiniLM-L6-v2`)\n",
        "2.  **Create our Knowledge Base**\n",
        "3.  **Encode Everything into Embeddings**\n",
        "4.  **Calculate Similarity** to find the best match"
      ],
      "metadata": {
        "id": "99P0U4tJzjq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import pipeline\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "\n",
        "# 1. Load our embedding model\n",
        "retriever_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Create a simple knowledge base\n",
        "knowledge_base = [\n",
        "    \"The capital of France is Paris, a city famous for the Eiffel Tower and the Louvre museum.\",\n",
        "    \"The Amazon rainforest is the world's largest tropical rainforest, known for its incredible biodiversity.\",\n",
        "    \"Mount Everest is the highest mountain on Earth, located in the Himalayas.\",\n",
        "    \"The Great Wall of China is a series of fortifications stretching over 13,000 miles.\",\n",
        "    \"Photosynthesis is the process used by plants to convert light energy into chemical energy.\"\n",
        "]\n",
        "\n",
        "# 3. Encode our knowledge base into embeddings\n",
        "knowledge_embeddings = retriever_model.encode(knowledge_base, convert_to_tensor=True)\n",
        "\n",
        "print(f\"âœ… Retriever model loaded and knowledge base encoded with {len(knowledge_base)} documents.\")"
      ],
      "metadata": {
        "id": "CJHpc394zkYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## âœï¸ Part 2: The Generator - Extracting the Answer\n",
        "\n",
        "Now we set up our Generator. This model will take the question and the context found by the retriever and extract the exact answer from it."
      ],
      "metadata": {
        "id": "Plr7LaDsznpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load our question-answering (generator) model\n",
        "generator = pipeline('question-answering', model='distilbert-base-cased-distilled-squad')\n",
        "\n",
        "print(\"âœ… Generator (QA) model loaded.\")"
      ],
      "metadata": {
        "id": "sCHCTi1xzpto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸš€ Part 3: Testing our RAG System\n",
        "\n",
        "Time to put it all together! The function below will simulate a full RAG pipeline and grade itself against a predefined set of questions and answers.\n",
        "\n",
        "It will test two key things:\n",
        "1.  **Retrieval Accuracy**: Did we find the right document?\n",
        "2.  **Generation Accuracy**: Did we extract the correct answer from that document?"
      ],
      "metadata": {
        "id": "kJMFmEjgzsIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_rag_assessment():\n",
        "    \"\"\"Runs a self-assessment of the RAG pipeline with multiple questions.\"\"\"\n",
        "\n",
        "    # Define our questions, expected context keywords, and expected answers\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the highest mountain?\",\n",
        "            \"expected_keyword\": \"Everest\",\n",
        "            \"expected_answer\": \"Mount Everest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which city is home to the Louvre museum?\",\n",
        "            \"expected_keyword\": \"France\",\n",
        "            \"expected_answer\": \"Paris\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"What process do plants use for energy?\",\n",
        "            \"expected_keyword\": \"Photosynthesis\",\n",
        "            \"expected_answer\": \"Photosynthesis\"\n",
        "        },\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2 # 2 points per question (1 for retrieval, 1 for generation)\n",
        "\n",
        "    print(\"--- ğŸš€ Starting RAG System Assessment ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "\n",
        "        # --- 1. Retrieval Step ---\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "\n",
        "        print(f\"ğŸ”  Retrieved Context: '{retrieved_context}'\")\n",
        "\n",
        "        # Check if the retrieval was correct\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"âœ…  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "\n",
        "        # --- 2. Generation Step ---\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "\n",
        "        print(f\"âœï¸  Generated Answer: '{generated_answer}'\")\n",
        "\n",
        "        # Check if the generation was correct\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"âœ…  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    # --- Final Score ---\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ‰ğŸ‰ğŸ‰ Perfect! Our RAG system is working as expected!\")\n",
        "    elif score >= total / 2:\n",
        "        print(\"ğŸ‘ Good job! The system is mostly correct.\")\n",
        "    else:\n",
        "        print(\"ğŸ”§ The system ran into some issues\")\n",
        "\n",
        "# Run the assessment!\n",
        "run_rag_assessment()"
      ],
      "metadata": {
        "id": "OvP6XETQztxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TASKS ğŸ§‘â€ğŸ’»"
      ],
      "metadata": {
        "id": "bmsraUmd0ruq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1 (Challenge): Add a New Question\n",
        "\n",
        "The task is to test the system with a new question about the **existing knowledge**."
      ],
      "metadata": {
        "id": "25iQeOoo6jTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 2: Adding new question to the assessment function\n",
        "\n",
        "\n",
        "def run_rag_assessment_task_2():\n",
        "    test_questions = [\n",
        "        {\n",
        "            \"question\": \"What is the world's largest tropical rainforest?\",\n",
        "            \"expected_keyword\": \"Amazon\",\n",
        "            \"expected_answer\": \"Amazon rainforest\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"How long is the Great Wall of China?\",\n",
        "            \"expected_keyword\": \"Great Wall of China\",\n",
        "            \"expected_answer\": \"13,000 miles\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Where is Mount Everest located?\",\n",
        "            \"expected_keyword\": \"Himalayas\",\n",
        "            \"expected_answer\": \"Himalayas\"\n",
        "        },\n",
        "        {\n",
        "            \"question\": \"Which museum is located in Paris and is famous worldwide?\",\n",
        "            \"expected_keyword\": \"Louvre\",\n",
        "            \"expected_answer\": \"Louvre museum\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- ğŸš€ Starting RAG System Assessment (Task 2) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings)[0]\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base[top_result_index]\n",
        "        print(f\"ğŸ”  Retrieved Context: '{retrieved_context}'\")\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"âœ…  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "        print(f\"âœï¸  Generated Answer: '{generated_answer}'\")\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"âœ…  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ‰ğŸ‰ğŸ‰ Perfect! O RAG system handled the new question!\")\n",
        "\n",
        "# Run the updated assessment\n",
        "run_rag_assessment_task_2()"
      ],
      "metadata": {
        "id": "6TKb4uBO6pEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3 (Advanced Challenge): Add New Knowledge & Test It\n",
        "\n",
        "Your final and most important task is to **expand the RAG system's knowledge base** and then test it.\n",
        "\n",
        "**Instructions:**\n",
        "1.  **Add a new fact** to the `knowledge_base` in the code cell below.\n",
        "2.  **You must re-run this cell** to update the `knowledge_embeddings`! The system won't know about the new fact until you do.\n",
        "3.  Finally, run the last code cell, which has a new test question about the knowledge you just added."
      ],
      "metadata": {
        "id": "JNHQuccw7Duu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 1: Add a new sentence to the knowledge base\n",
        "\n",
        "knowledge_base_task_3 = [\n",
        "    \"The Sahara Desert is the largest hot desert in the world, located in North Africa.\"\n",
        "]\n",
        "\n",
        "# Re-encode the updated knowledge base\n",
        "knowledge_embeddings_task_3 = retriever_model.encode(knowledge_base_task_3, convert_to_tensor=True)\n",
        "\n",
        "print(f\"âœ… Knowledge base updated and re-encoded with {len(knowledge_base_task_3)} documents.\")"
      ],
      "metadata": {
        "id": "4ijwpN1W7VyK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Task 3, Step 2: Test your newly added knowledge\n",
        "\n",
        "def run_rag_assessment_task_3():\n",
        "    test_questions = [\n",
        "        # --- NEW QUESTION FOR YOUR NEW KNOWLEDGE ---\n",
        "        {\n",
        "            \"question\": \"What is the largest hot desert in the world?\",\n",
        "            \"expected_keyword\": \"Sahara Desert\",\n",
        "            \"expected_answer\": \"The Sahara Desert\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    score = 0\n",
        "    total = len(test_questions) * 2\n",
        "\n",
        "    print(\"--- ğŸš€ Starting RAG System Assessment (Task 3) ---\\n\")\n",
        "\n",
        "    for i, test in enumerate(test_questions):\n",
        "        question = test[\"question\"]\n",
        "        print(f\"\\n--- Question {i+1}: '{question}' ---\")\n",
        "        # Use the updated embeddings from Task 3\n",
        "        question_embedding = retriever_model.encode(question, convert_to_tensor=True)\n",
        "        cos_scores = util.pytorch_cos_sim(question_embedding, knowledge_embeddings_task_3)[0]\n",
        "        # Use the updated knowledge base from Task 3\n",
        "        top_result_index = torch.argmax(cos_scores)\n",
        "        retrieved_context = knowledge_base_task_3[top_result_index]\n",
        "        print(f\"ğŸ”  Retrieved Context: '{retrieved_context}'\")\n",
        "        if test[\"expected_keyword\"] in retrieved_context:\n",
        "            print(\"âœ…  Retrieval Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Retrieval Failed. Expected context with keyword: '{test['expected_keyword']}'\")\n",
        "        qa_result = generator(question=question, context=retrieved_context)\n",
        "        generated_answer = qa_result['answer']\n",
        "        print(f\"âœï¸  Generated Answer: '{generated_answer}'\")\n",
        "        if test[\"expected_answer\"].lower() in generated_answer.lower():\n",
        "            print(\"âœ…  Generation Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            print(f\"âŒ  Generation Failed. Expected answer: '{test['expected_answer']}'\")\n",
        "\n",
        "    print(f\"\\n--- ğŸ Assessment Complete ---\")\n",
        "    print(f\"ğŸ¯ Final Score: {score} / {total}\")\n",
        "    if score == total:\n",
        "        print(\"ğŸ†ğŸ†ğŸ† Success! You have successfully extended the knowledge of your RAG system!\")\n",
        "\n",
        "# Run the final assessment\n",
        "run_rag_assessment_task_3()"
      ],
      "metadata": {
        "id": "-tgQJ13P7pCs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}